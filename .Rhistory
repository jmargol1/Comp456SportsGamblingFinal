set_engine(engine = 'rpart') %>%
set_args(cost_complexity = NULL,  #default is 0.01 (used for pruning a tree)
min_n = NULL, #min number of observations to try split: default is 20 [I think the documentation has a typo and says 2]  (used to stop early)
tree_depth = NULL) %>% #max depth, number of branches/splits to get to any final group: default is 30 (used to stop early)
set_mode('classification') # change this for regression tree
data_rec <- recipe(make_bet ~ money_line + spread + total + team_total + Location + book_id, data = NFL_Full)
data_wf <- workflow() %>%
add_model(ct_spec) %>%
add_recipe(data_rec)
fit_mod <- data_wf %>% # or use tune_grid() to tune any of the parameters above
fit(data = NFL_Full)
# Plot the tree (make sure to load the rpart.plot package first)
fit_mod %>%
extract_fit_engine() %>%
rpart.plot()
# Get variable importance metrics
# Sum of the goodness of split measures (impurity reduction) for each split for which it was the primary variable.
fit_mod %>%
extract_fit_engine() %>%
pluck('variable.importance')
ct_spec <- decision_tree() %>%
set_engine(engine = 'rpart') %>%
set_args(cost_complexity = 0.01,  #default is 0.01 (used for pruning a tree)
min_n = NULL, #min number of observations to try split: default is 20 [I think the documentation has a typo and says 2]  (used to stop early)
tree_depth = NULL) %>% #max depth, number of branches/splits to get to any final group: default is 30 (used to stop early)
set_mode('classification') # change this for regression tree
data_rec <- recipe(make_bet ~ money_line + spread + total + team_total + Location + book_id, data = NFL_Full)
data_wf <- workflow() %>%
add_model(ct_spec) %>%
add_recipe(data_rec)
fit_mod <- data_wf %>% # or use tune_grid() to tune any of the parameters above
fit(data = NFL_Full)
# Plot the tree (make sure to load the rpart.plot package first)
fit_mod %>%
extract_fit_engine() %>%
rpart.plot()
# Get variable importance metrics
# Sum of the goodness of split measures (impurity reduction) for each split for which it was the primary variable.
fit_mod %>%
extract_fit_engine() %>%
pluck('variable.importance')
ct_spec <- decision_tree() %>%
set_engine(engine = 'rpart') %>%
set_args(cost_complexity = 0.01,  #default is 0.01 (used for pruning a tree)
min_n = 6, #min number of observations to try split: default is 20 [I think the documentation has a typo and says 2]  (used to stop early)
tree_depth = NULL) %>% #max depth, number of branches/splits to get to any final group: default is 30 (used to stop early)
set_mode('classification') # change this for regression tree
data_rec <- recipe(make_bet ~ money_line + spread + total + team_total + Location + book_id, data = NFL_Full)
data_wf <- workflow() %>%
add_model(ct_spec) %>%
add_recipe(data_rec)
fit_mod <- data_wf %>% # or use tune_grid() to tune any of the parameters above
fit(data = NFL_Full)
# Plot the tree (make sure to load the rpart.plot package first)
fit_mod %>%
extract_fit_engine() %>%
rpart.plot()
# Get variable importance metrics
# Sum of the goodness of split measures (impurity reduction) for each split for which it was the primary variable.
fit_mod %>%
extract_fit_engine() %>%
pluck('variable.importance')
ct_spec <- decision_tree() %>%
set_engine(engine = 'rpart') %>%
set_args(cost_complexity = NULL,  #default is 0.01 (used for pruning a tree)
min_n = 6, #min number of observations to try split: default is 20 [I think the documentation has a typo and says 2]  (used to stop early)
tree_depth = NULL) %>% #max depth, number of branches/splits to get to any final group: default is 30 (used to stop early)
set_mode('classification') # change this for regression tree
data_rec <- recipe(make_bet ~ money_line + spread + total + team_total + Location + book_id, data = NFL_Full)
data_wf <- workflow() %>%
add_model(ct_spec) %>%
add_recipe(data_rec)
fit_mod <- data_wf %>% # or use tune_grid() to tune any of the parameters above
fit(data = NFL_Full)
# Plot the tree (make sure to load the rpart.plot package first)
fit_mod %>%
extract_fit_engine() %>%
rpart.plot()
# Get variable importance metrics
# Sum of the goodness of split measures (impurity reduction) for each split for which it was the primary variable.
fit_mod %>%
extract_fit_engine() %>%
pluck('variable.importance')
ct_spec <- decision_tree() %>%
set_engine(engine = 'rpart') %>%
set_args(cost_complexity = NULL,  #default is 0.01 (used for pruning a tree)
#min number of observations to try split: default is 20 [I think the documentation has a typo and says 2]  (used to stop early)
tree_depth = NULL) %>% #max depth, number of branches/splits to get to any final group: default is 30 (used to stop early)
set_mode('classification') # change this for regression tree
data_rec <- recipe(make_bet ~ money_line + spread + total + team_total + Location + book_id, data = NFL_Full)
data_wf <- workflow() %>%
add_model(ct_spec) %>%
add_recipe(data_rec)
fit_mod <- data_wf %>% # or use tune_grid() to tune any of the parameters above
fit(data = NFL_Full)
# Plot the tree (make sure to load the rpart.plot package first)
fit_mod %>%
extract_fit_engine() %>%
rpart.plot()
# Get variable importance metrics
# Sum of the goodness of split measures (impurity reduction) for each split for which it was the primary variable.
fit_mod %>%
extract_fit_engine() %>%
pluck('variable.importance')
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(tidymodels)
library(vip)
library(probably) #install.packages('probably')
tidymodels_prefer()
conflicted::conflict_prefer("vi", "vip")
library(rpart.plot)
library(plotly)
set.seed(121)
load("NFL_movement_split_df.rdata")
NFL_Full <- NFL_lines_split %>%
select(-Index, -inserted, -league, -id, -Final_line, -public_team_under) %>%
mutate(book_id = as.factor(book_id)) %>%
filter(book_id != "69")%>%
na.omit()
NFL_Full %>%
group_by(factor(make_bet)) %>%
summarise(t=n())
NFL_lines_split %>%
group_by(book_id) %>%
summarise(t = n())
# Make sure you set reference level (the outcome you are NOT interested in)
NFL_Full <- NFL_Full %>%
mutate(outcome = relevel(factor(make_bet), ref='No')) #set reference level
NFL_cv10 <- vfold_cv(NFL_Full, v = 10)
# Logistic Regression Model Spec
logistic_spec <- logistic_reg() %>%
set_engine('glm') %>%
set_mode('classification')
# Recipe
logistic_rec <- recipe(make_bet ~ money_line + spread + total + team_total + Location + book_id, data = NFL_Full, family = binomial('logit'), maxit = 100) %>%
step_normalize(all_numeric_predictors()) %>%
step_dummy(all_nominal_predictors())
# Workflow (Recipe + Model)
log_wf <- workflow() %>%
add_recipe(logistic_rec) %>%
add_model(logistic_spec)
# Fit Model to Training Data
log_fit <- fit(log_wf, data = NFL_Full)
# Print out Coefficients
log_fit %>% tidy()
# Get Exponentiated coefficients + CI
log_fit %>% tidy() %>%
mutate(OR.conf.low = exp(estimate - 1.96*std.error), OR.conf.high = exp(estimate + 1.96*std.error)) %>% # do this first
mutate(OR = exp(estimate))
# Make soft (probability) predictions
predict(log_fit, new_data = NFL_Full, type = "prob")
# Make hard (class) predictions (using a default 0.5 probability threshold)
predict(log_fit, new_data = NFL_Full, type = "class")
# Soft predictions
logistic_output <-  NFL_Full %>%
bind_cols(predict(log_fit, new_data = NFL_Full, type = 'prob'))
# Hard predictions (you pick threshold)
logistic_output <- logistic_output %>%
mutate(.pred_class = make_two_class_pred(.pred_No, levels(outcome), threshold = .60)) #Try changing threshold (.5, 0, 1, .2, .8)
# Visualize Soft Predictions
logistic_output %>%
ggplot(aes(x = outcome, y = .pred_Yes)) +
geom_boxplot() +
geom_hline(yintercept = 0.60, color='red') +  # try changing threshold
labs(y = 'Predicted Probability of Outcome', x = 'Observed Outcome') +
theme_classic()
# Confusion Matrix
logistic_output %>%
conf_mat(truth = outcome, estimate = .pred_class)
log_metrics <- metric_set(sens, yardstick::spec, accuracy) # these metrics are based on hard predictions
#sens: sensitivity = chance of correctly predicting second level, given second level (Yes)
#spec: specificity = chance of correctly predicting first level, given first level (No)
#accuracy: accuracy = chance of correctly predicting outcome
logistic_output %>%
log_metrics(estimate = .pred_class, truth = outcome, event_level = "second") # set second level of outcome as "success"
logistic_roc <- logistic_output %>%
roc_curve(outcome, .pred_Yes, event_level = "second") # set second level of outcome as "success"
autoplot(logistic_roc) + theme_classic()
# CV Fit Model
log_cv_fit <- fit_resamples(
log_wf,
resamples = NFL_cv10,
metrics = metric_set(sens, yardstick::spec, accuracy, roc_auc),
control = control_resamples(save_pred = TRUE, event_level = 'second'))  # you need predictions for ROC calculations
collect_metrics(log_cv_fit) #default threshold is 0.5
rf_OOB_output(data_fit_mtry4,4, NFL_Full %>% pull(make_bet)) %>%
conf_mat(truth = make_bet, estimate= .pred_class)
model_output <-data_fit_mtry4 %>%
extract_fit_engine()
model_output %>%
vip(num_features = 30) + theme_classic() #based on impurity
model_output %>% vip::vi() %>% head()
model_output2 <- data_wf_mtry4 %>%
update_model(rf_spec %>% set_args(importance = "permutation")) %>% #based on permutation
fit(data = NFL_Full) %>%
extract_fit_engine()
ct_spec <- decision_tree() %>%
set_engine(engine = 'rpart') %>%
set_args(cost_complexity = 0.00000001,  #default is 0.01 (used for pruning a tree)
min_n = NULL, #min number of observations to try split: default is 20 [I think the documentation has a typo and says 2]  (used to stop early)
tree_depth = NULL) %>% #max depth, number of branches/splits to get to any final group: default is 30 (used to stop early)
set_mode('classification') # change this for regression tree
data_rec <- recipe(make_bet ~ money_line + spread + total + team_total + Location + book_id, data = NFL_Full)
data_wf <- workflow() %>%
add_model(ct_spec) %>%
add_recipe(data_rec)
fit_mod <- data_wf %>% # or use tune_grid() to tune any of the parameters above
fit(data = NFL_Full)
# Plot the tree (make sure to load the rpart.plot package first)
fit_mod %>%
extract_fit_engine() %>%
rpart.plot(model = TRUE)
# Plot the tree (make sure to load the rpart.plot package first)
fit_mod %>%
extract_fit_engine() %>%
rpart.plot()
# Get variable importance metrics
# Sum of the goodness of split measures (impurity reduction) for each split for which it was the primary variable.
fit_mod %>%
extract_fit_engine() %>%
pluck('variable.importance')
ct_spec <- decision_tree() %>%
set_engine(engine = 'rpart') %>%
set_args(cost_complexity = 0.001,  #default is 0.01 (used for pruning a tree)
min_n = NULL, #min number of observations to try split: default is 20 [I think the documentation has a typo and says 2]  (used to stop early)
tree_depth = NULL) %>% #max depth, number of branches/splits to get to any final group: default is 30 (used to stop early)
set_mode('classification') # change this for regression tree
data_rec <- recipe(make_bet ~ money_line + spread + total + team_total + Location + book_id, data = NFL_Full)
data_wf <- workflow() %>%
add_model(ct_spec) %>%
add_recipe(data_rec)
fit_mod <- data_wf %>% # or use tune_grid() to tune any of the parameters above
fit(data = NFL_Full)
# Plot the tree (make sure to load the rpart.plot package first)
fit_mod %>%
extract_fit_engine() %>%
rpart.plot()
# Get variable importance metrics
# Sum of the goodness of split measures (impurity reduction) for each split for which it was the primary variable.
fit_mod %>%
extract_fit_engine() %>%
pluck('variable.importance')
ct_spec <- decision_tree() %>%
set_engine(engine = 'rpart') %>%
set_args(cost_complexity = 0.01,  #default is 0.01 (used for pruning a tree)
min_n = NULL, #min number of observations to try split: default is 20 [I think the documentation has a typo and says 2]  (used to stop early)
tree_depth = NULL) %>% #max depth, number of branches/splits to get to any final group: default is 30 (used to stop early)
set_mode('classification') # change this for regression tree
data_rec <- recipe(make_bet ~ money_line + spread + total + team_total + Location + book_id, data = NFL_Full)
data_wf <- workflow() %>%
add_model(ct_spec) %>%
add_recipe(data_rec)
fit_mod <- data_wf %>% # or use tune_grid() to tune any of the parameters above
fit(data = NFL_Full)
# Plot the tree (make sure to load the rpart.plot package first)
fit_mod %>%
extract_fit_engine() %>%
rpart.plot()
# Get variable importance metrics
# Sum of the goodness of split measures (impurity reduction) for each split for which it was the primary variable.
fit_mod %>%
extract_fit_engine() %>%
pluck('variable.importance')
ct_spec <- decision_tree() %>%
set_engine(engine = 'rpart') %>%
set_args(cost_complexity = 0.015,  #default is 0.01 (used for pruning a tree)
min_n = NULL, #min number of observations to try split: default is 20 [I think the documentation has a typo and says 2]  (used to stop early)
tree_depth = NULL) %>% #max depth, number of branches/splits to get to any final group: default is 30 (used to stop early)
set_mode('classification') # change this for regression tree
data_rec <- recipe(make_bet ~ money_line + spread + total + team_total + Location + book_id, data = NFL_Full)
data_wf <- workflow() %>%
add_model(ct_spec) %>%
add_recipe(data_rec)
fit_mod <- data_wf %>% # or use tune_grid() to tune any of the parameters above
fit(data = NFL_Full)
# Plot the tree (make sure to load the rpart.plot package first)
fit_mod %>%
extract_fit_engine() %>%
rpart.plot()
# Get variable importance metrics
# Sum of the goodness of split measures (impurity reduction) for each split for which it was the primary variable.
fit_mod %>%
extract_fit_engine() %>%
pluck('variable.importance')
ct_spec <- decision_tree() %>%
set_engine(engine = 'rpart') %>%
set_args(cost_complexity = 0.0075,  #default is 0.01 (used for pruning a tree)
min_n = NULL, #min number of observations to try split: default is 20 [I think the documentation has a typo and says 2]  (used to stop early)
tree_depth = NULL) %>% #max depth, number of branches/splits to get to any final group: default is 30 (used to stop early)
set_mode('classification') # change this for regression tree
data_rec <- recipe(make_bet ~ money_line + spread + total + team_total + Location + book_id, data = NFL_Full)
data_wf <- workflow() %>%
add_model(ct_spec) %>%
add_recipe(data_rec)
fit_mod <- data_wf %>% # or use tune_grid() to tune any of the parameters above
fit(data = NFL_Full)
# Plot the tree (make sure to load the rpart.plot package first)
fit_mod %>%
extract_fit_engine() %>%
rpart.plot()
# Get variable importance metrics
# Sum of the goodness of split measures (impurity reduction) for each split for which it was the primary variable.
fit_mod %>%
extract_fit_engine() %>%
pluck('variable.importance')
ct_spec <- decision_tree() %>%
set_engine(engine = 'rpart') %>%
set_args(cost_complexity = 0.005,  #default is 0.01 (used for pruning a tree)
min_n = NULL, #min number of observations to try split: default is 20 [I think the documentation has a typo and says 2]  (used to stop early)
tree_depth = NULL) %>% #max depth, number of branches/splits to get to any final group: default is 30 (used to stop early)
set_mode('classification') # change this for regression tree
data_rec <- recipe(make_bet ~ money_line + spread + total + team_total + Location + book_id, data = NFL_Full)
data_wf <- workflow() %>%
add_model(ct_spec) %>%
add_recipe(data_rec)
fit_mod <- data_wf %>% # or use tune_grid() to tune any of the parameters above
fit(data = NFL_Full)
# Plot the tree (make sure to load the rpart.plot package first)
fit_mod %>%
extract_fit_engine() %>%
rpart.plot()
# Get variable importance metrics
# Sum of the goodness of split measures (impurity reduction) for each split for which it was the primary variable.
fit_mod %>%
extract_fit_engine() %>%
pluck('variable.importance')
model_output2 <- data_wf_mtry4 %>%
update_model(rf_spec %>% set_args(importance = "permutation")) %>% #based on permutation
fit(data = NFL_Full) %>%
extract_fit_engine()
model_output2 %>%
vip(num_features = 30) + theme_classic()
model_output2 %>% vip::vi() %>% head()
install.packages("rfviz")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(tidymodels)
library(vip)
library(probably) #install.packages('probably')
tidymodels_prefer()
conflicted::conflict_prefer("vi", "vip")
library(rpart.plot)
library(plotly)
library(rfviz)
knitr::opts_chunk$set(echo = TRUE)
library(cfbfastR)
library(dplyr)
library(ggplot2)
library(plotly)
library(tidyverse)
CFBD_API_KEY = "ysMTW4jIsv4kWgHXoM3yF8lVKm+4wNfuM9VKnZlrT5kavH0nXCxSyUpNGn4LKdtE"
Sys.setenv(CFBD_API_KEY = CFBD_API_KEY)
cfb.betting.2021 = cfbfastR::cfbd_betting_lines(year = 2021)
team_in_conference = function(conference, home_conference, away_conference) {
return(conference == home_conference | conference == away_conference)
}
unique(cfb.betting.2021$home_conference)[1:11]
CFBConf <- cfb.betting.2021 %>%
mutate(mac = team_in_conference("Mid-American", home_conference, away_conference),
aac = team_in_conference("American Athletic", home_conference, away_conference),
acc = team_in_conference("ACC", home_conference, away_conference),
mtw = team_in_conference("Mountain West", home_conference, away_conference),
big10 = team_in_conference("Big Ten", home_conference, away_conference),
pac12 = team_in_conference("Pac-12", home_conference, away_conference),
independent = team_in_conference("FBS Independents", home_conference, away_conference),
big12 = team_in_conference("Big 12", home_conference, away_conference),
sbc = team_in_conference("Sun Belt", home_conference, away_conference),
sec = team_in_conference("SEC", home_conference, away_conference),
cusa = team_in_conference("Conference USA", home_conference, away_conference))
CFBConf <- CFBConf[ !is.na(CFBConf$spread), ]
mac <- CFBConf %>%
filter(mac == TRUE) %>%
summarise("Conference" = "Mid-American", "Spread" = mean(as.numeric(spread)), "Outcome" = mean(away_score - home_score))
aac <- CFBConf %>%
filter(aac == TRUE) %>%
summarise("Conference" = "American Athletic", "Spread" = mean(as.numeric(spread)), "Outcome" = mean(away_score - home_score))
acc <- CFBConf %>%
filter(acc == TRUE) %>%
summarise("Conference" = "ACC", "Spread" = mean(as.numeric(spread)), "Outcome" = mean(away_score - home_score))
mtw <- CFBConf %>%
filter(mtw == TRUE) %>%
summarise("Conference" = "Mountain West", "Spread" = mean(as.numeric(spread)), "Outcome" = mean(away_score - home_score))
big10 <- CFBConf %>%
filter(big10 == TRUE) %>%
summarise("Conference" = "Big 10", "Spread" = mean(as.numeric(spread)), "Outcome" = mean(away_score - home_score))
pac12 <- CFBConf %>%
filter(pac12 == TRUE) %>%
summarise("Conference" = "Pac-12", "Spread" = mean(as.numeric(spread)), "Outcome" = mean(away_score - home_score))
independent <- CFBConf %>%
filter(independent == TRUE) %>%
summarise("Conference" = "Independent", "Spread" = mean(as.numeric(spread)), "Outcome" = mean(away_score - home_score))
big12 <- CFBConf %>%
filter(big12 == TRUE) %>%
summarise("Conference" = "Big 12", "Spread" = mean(as.numeric(spread)), "Outcome" = mean(away_score - home_score))
sbc <- CFBConf %>%
filter(sbc == TRUE) %>%
summarise("Conference" = "Sun Belt", "Spread" = mean(as.numeric(spread)), "Outcome" = mean(away_score - home_score))
sec <- CFBConf %>%
filter(sec == TRUE) %>%
summarise("Conference" = "SEC", "Spread" = mean(as.numeric(spread)), "Outcome" = mean(away_score - home_score))
cusa <- CFBConf %>%
filter(cusa == TRUE) %>%
summarise("Conference" = "Conference USA", "Spread" = mean(as.numeric(spread)), "Outcome" = mean(away_score - home_score))
Spreads <- mac %>%
rbind(aac)
Spreads <- Spreads %>%
rbind(acc)
Spreads <- Spreads %>%
rbind(big10)
Spreads <- Spreads %>%
rbind(mtw)
Spreads <- Spreads %>%
rbind(pac12)
Spreads <- Spreads %>%
rbind(independent)
Spreads <- Spreads %>%
rbind(big12)
Spreads <- Spreads %>%
rbind(sbc)
Spreads <- Spreads %>%
rbind(sec)
Spreads <- Spreads %>%
rbind(cusa)
Spreads <- Spreads %>%
pivot_longer(cols=c(`Spread`, `Outcome`),names_to="Spread/Outcome",values_to="Average")
Spreads$Conference <- factor(Spreads$Conference, levels = c("SEC", "Mid-American", "Conference USA", "Big 10", "Mountain West", "Big 12", "ACC", "Pac-12", "Independent", "American Athletic", "Sun Belt"))
vis <- ggplot(Spreads, aes(x = Conference, y = - `Average`, fill = `Spread/Outcome`)) +
geom_col(position = "dodge") +
labs(y = "Average Score Difference", title = "Average Difference Between Spread and Outcomes By Conference") +
theme(axis.text.x = element_text(angle = 45, hjust=1)) +
theme_minimal()
ggplotly(vis) %>%
layout(xaxis = list(tickangle = 45, tickfont  = 22, titlefont = 22))
cfb = CFBConf %>%
group_by(game_id) %>%
summarise(spraed = mean(as.numeric(spread))) %>%
ungroup()
cfb = CFBConf %>%
mutate(non_conf = home_conference != away_conference,
home_spread = as.numeric(spread),
away_spread = -as.numeric(spread))
conferences = unique(cfb.betting.2021$home_conference)[1:11]
cfb %>%
group_by(home_conference) %>%
summarise(avg.spread = mean(home_spread))
cfb %>%
group_by(away_conference) %>%
summarise(avg.spraed = mean(away_spread))
cfb[,""]
library(ggplot2)
library(dplyr)
cfb = cfb.betting.2021 %>%
filter(!is.na(spread_open))
cfb.home = cfb %>%
select(home_conference, spread) %>%
mutate(spread = as.numeric(spread),
home = 1)
names(cfb.home)[names(cfb.home) == "home_conference"] = "conference"
cfb.away = cfb %>%
select(away_conference, spread) %>%
mutate(spread = -as.numeric(spread),
home = 0)
names(cfb.away)[names(cfb.away) == "away_conference"] = "conference"
cfb.away
cfb.home
fbs.conferences = unique(cfb.betting.2021$home_conference)[1:11]
cfb.spread = rbind(cfb.away, cfb.home) %>%
mutate(conference = ifelse(conference %in% fbs.conferences, conference, "FCS"))
cfb.spread %>%
group_by(conference) %>%
summarise(avg.spread = mean(spread))
vis = cfb.spread %>%
group_by(conference) %>%
summarise(avg.spread = mean(spread)) %>%
ggplot(aes(x = conference, y = avg.spread)) +
geom_col()
vis
cfb = cfb %>%
mutate(nonconf = home_conference != away_conference)
OUConf <- cfb %>%
filter(home_conference == away_conference) %>%
mutate(total = away_score+home_score) %>%
mutate(OverHit = ifelse(total > over_under, 1, 0)) %>%
mutate(over_under = as.numeric(over_under)) %>%
mutate( OverCat = cut(over_under , c(30,35,40,45,50,55,60,65,max(over_under)))) %>%
group_by(home_conference, OverCat) %>%
summarise("HitPercent" = sum(OverHit/n()), "total" = n())
OUConfVis <- ggplot(OUConf, aes(x=OverCat, y=HitPercent)) +
geom_col(color="white") +
facet_wrap( ~ home_conference) +
labs(xlab = "Over-Under Range", ylab = "Percent of Overs that Hit") +
theme_minimal()
ggplotly(OUConfVis)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(tidymodels)
library(vip)
library(probably) #install.packages('probably')
tidymodels_prefer()
conflicted::conflict_prefer("vi", "vip")
library(rpart.plot)
library(plotly)
library(rfviz)
library(tree.interpreter)
library(kableExtra)
load("consensus_data.rdata")
