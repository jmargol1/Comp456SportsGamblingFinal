---
title: "NFL CLV Analysis"
author: "Joe Margolis"
date: '2022-11-08'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(dplyr)
library(ggplot2)
library(tidymodels)
library(vip)
library(probably) #install.packages('probably')
tidymodels_prefer()
conflicted::conflict_prefer("vi", "vip")
library(rpart.plot)
library(plotly)
library(rfviz)
library(tree.interpreter)

set.seed(121)

load("NFL_Data/NFL_movement_split_df.rdata")
```

# Home Analysis

## Logistic Regression 

### Final Cleaning

```{r}
NFL_Sample <- NFL_lines_split %>%
  select(-Index, -Final_line, -inserted, -public_ml, -public_spread, -public_team_under, -public_team_over) %>%
  na.omit() %>%
  group_by(id, book_id, Location) %>%
  sample_n(1)

NFL_Home <- NFL_Sample %>%
  filter(Location == 'home')

NFL_Home = subset(NFL_Home, select = -c(id,book_id, league, Location) )

```

### Model Setting

```{r}
# Make sure you set reference level (the outcome you are NOT interested in)
NFL_Home <- NFL_Home %>%
  mutate(outcome = relevel(factor(make_bet), ref='No')) #set reference level

NFL_cv10 <- vfold_cv(NFL_Home, v = 10)

# Logistic Regression Model Spec
logistic_spec <- logistic_reg() %>%
    set_engine('glm') %>%
    set_mode('classification')

# Recipe
logistic_rec <- recipe(make_bet ~ money_line + spread + total + team_total, data = NFL_Home, family = binomial('logit'), maxit = 100) %>%
    step_normalize(all_numeric_predictors()) %>% 
    step_dummy(all_nominal_predictors())

# Workflow (Recipe + Model)
log_wf <- workflow() %>% 
    add_recipe(logistic_rec) %>%
    add_model(logistic_spec) 

# Fit Model to Training Data
log_fit <- fit(log_wf, data = NFL_Home)
```

### Checking Model

```{r}
# Print out Coefficients
log_fit %>% tidy()

# Get Exponentiated coefficients + CI
log_fit %>% tidy() %>%
  mutate(OR.conf.low = exp(estimate - 1.96*std.error), OR.conf.high = exp(estimate + 1.96*std.error)) %>% # do this first
  mutate(OR = exp(estimate))
```

### Predicting on all Data
```{r}
# Make soft (probability) predictions
predict(log_fit, new_data = NFL_Home, type = "prob")

# Make hard (class) predictions (using a default 0.5 probability threshold)
predict(log_fit, new_data = NFL_Home, type = "class")
```

### Predicting on Training Data

```{r}
# Soft predictions
logistic_output <-  NFL_Home %>%
  bind_cols(predict(log_fit, new_data = NFL_Home, type = 'prob')) 

# Hard predictions (you pick threshold)
logistic_output <- logistic_output %>%
  mutate(.pred_class = make_two_class_pred(.pred_No, levels(outcome), threshold = .60)) #Try changing threshold (.5, 0, 1, .2, .8)

# Visualize Soft Predictions
logistic_output %>%
  ggplot(aes(x = outcome, y = .pred_Yes)) +
  geom_boxplot() + 
  geom_hline(yintercept = 0.60, color='red') +  # try changing threshold
  labs(y = 'Predicted Probability of Outcome', x = 'Observed Outcome') +
  theme_classic()
```

### Evaluating Model

```{r}
# Confusion Matrix
logistic_output %>%
  conf_mat(truth = outcome, estimate = .pred_class)

log_metrics <- metric_set(sens, yardstick::spec, accuracy) # these metrics are based on hard predictions

#sens: sensitivity = chance of correctly predicting second level, given second level (Yes)
#spec: specificity = chance of correctly predicting first level, given first level (No)
#accuracy: accuracy = chance of correctly predicting outcome

logistic_output %>% 
  log_metrics(estimate = .pred_class, truth = outcome, event_level = "second") # set second level of outcome as "success"
```

### ROC Curve

```{r}
logistic_roc <- logistic_output %>% 
    roc_curve(outcome, .pred_Yes, event_level = "second") # set second level of outcome as "success"

autoplot(logistic_roc) + theme_classic()
```

### CV Evaluation of Model

```{r}
# CV Fit Model
log_cv_fit <- fit_resamples(
    log_wf, 
    resamples = NFL_cv10,
    metrics = metric_set(sens, yardstick::spec, accuracy, roc_auc),
    control = control_resamples(save_pred = TRUE, event_level = 'second'))  # you need predictions for ROC calculations

collect_metrics(log_cv_fit) #default threshold is 0.5
```

## Random Forest Analysis

### Building Forest

```{r}
# Model Specification
rf_spec <- rand_forest() %>%
  set_engine(engine = 'ranger') %>% 
  set_args(mtry = NULL, # size of random subset of variables; default is floor(sqrt(number of total predictors))
           trees = 1000, # Number of trees
           min_n = 2,
           probability = FALSE, # FALSE: get hard predictions (not needed for regression)
           importance = 'impurity') %>% # we'll come back to this at the end
  set_mode('classification') # change this for regression

# Recipe
data_rec <- recipe(make_bet ~ spread + total + team_total + money_line, data = NFL_Home)

# Workflows
data_wf_mtry2 <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 2)) %>%
  add_recipe(data_rec)

## Create workflows for mtry = 12, 74, and 147

data_wf_mtry4 <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 4)) %>%
  add_recipe(data_rec)

"data_wf_mtry6 <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 6)) %>%
  add_recipe(data_rec)"

```

### Fitting Models

```{r}
NFL_Home <- NFL_Home %>%
  na.omit()

set.seed(121) # make sure to run this before each fit so that you have the same 1000 trees
data_fit_mtry2 <- fit(data_wf_mtry2, data = NFL_Home)

```

```{r}

# Fit models for 12, 74, 147
set.seed(121) 
data_fit_mtry4 <- fit(data_wf_mtry4, data = NFL_Home)

"set.seed(121)
data_fit_mtry6 <- fit(data_wf_mtry6, data = NFL_Home)"
```

```{r}
# Custom Function to get OOB predictions, true observed outcomes and add a user-provided model label
rf_OOB_output <- function(fit_model, model_label, truth){
    tibble(
          .pred_class = fit_model %>% extract_fit_engine() %>% pluck('predictions'), #OOB predictions
          make_bet = truth,
          label = model_label
      )
}

#check out the function output
#rf_OOB_output(data_fit_mtry6,6, NFL_Home %>% pull(make_bet))
rf_OOB_output(data_fit_mtry4,4, NFL_Home %>% pull(make_bet))
rf_OOB_output(data_fit_mtry2,2, NFL_Home %>% pull(make_bet))
```

### Evaluating Models

```{r}
# Evaluate OOB Metrics

data_rf_OOB_output <- bind_rows(
    #rf_OOB_output(data_fit_mtry6,6, NFL_Home %>% pull(make_bet)),
    rf_OOB_output(data_fit_mtry4,4, NFL_Home %>% pull(make_bet)),
    rf_OOB_output(data_fit_mtry2,2, NFL_Home %>% pull(make_bet))
  
)


data_rf_OOB_output %>% 
    group_by(label) %>%
    accuracy(truth = factor(make_bet), estimate = .pred_class)
```

```{r}
data_rf_OOB_output %>%
  group_by(label) %>%
  accuracy(truth = factor(make_bet), estimate = .pred_class) %>%
  ggplot(aes(x = label, y = .estimate)) +
  geom_point() +
  geom_path()+
  scale_x_continuous(breaks = c(2, 4, 6))
```

### Evaluating Best Forest

```{r}
data_fit_mtry4
```
```{r, warning=FALSE}
mtry2Conf <- rf_OOB_output(data_fit_mtry2,2, NFL_Home %>% pull(make_bet)) %>%
    conf_mat(truth = make_bet, estimate= .pred_class)

mtry4Conf <- rf_OOB_output(data_fit_mtry4,4, NFL_Home %>% pull(make_bet)) %>%
    conf_mat(truth = make_bet, estimate= .pred_class)

mtry2Conf <- as.data.frame.matrix(mtry2Conf$table)
mtry4Conf <- as.data.frame.matrix(mtry4Conf$table)

mtry2Conf
mtry4Conf
```

### Variable Importance

```{r}
model_output <-data_fit_mtry4 %>% 
    extract_fit_engine() 

model_output %>% 
    vip(num_features = 30) + theme_classic() #based on impurity

model_output %>% vip::vi() %>% head()
```

```{r}
model_output2 <- data_wf_mtry4 %>% 
  update_model(rf_spec %>% set_args(importance = "permutation")) %>% #based on permutation
  fit(data = NFL_Home) %>% 
    extract_fit_engine() 

model_output2 %>% 
    vip(num_features = 30) + theme_classic()


model_output2 %>% vip::vi() %>% head()

```

## Decision Tree

### Fitting Tree

```{r}
ct_spec <- decision_tree() %>%
  set_engine(engine = 'rpart') %>%
  set_args(cost_complexity = 0.005,  #default is 0.01 (used for pruning a tree)
           min_n = NULL, #min number of observations to try split: default is 20 [I think the documentation has a typo and says 2]  (used to stop early)
           tree_depth = NULL) %>% #max depth, number of branches/splits to get to any final group: default is 30 (used to stop early)
  set_mode('classification') # change this for regression tree


data_rec <- recipe(make_bet ~ money_line + spread + total + team_total , data = NFL_Home)

data_wf <- workflow() %>%
  add_model(ct_spec) %>%
  add_recipe(data_rec)

fit_mod <- data_wf %>% # or use tune_grid() to tune any of the parameters above
  fit(data = NFL_Home)
```

### Visualizing Tree

```{r}
# Plot the tree (make sure to load the rpart.plot package first)
fit_mod %>%
  extract_fit_engine() %>%
  rpart.plot()

# Get variable importance metrics 
# Sum of the goodness of split measures (impurity reduction) for each split for which it was the primary variable.
fit_mod %>%
  extract_fit_engine() %>%
  pluck('variable.importance')
```

```{r}
# Hard (class) prediction
prediction <- predict(fit_mod, new_data = NFL_Home, type = "class")
```

```{r}
table_mat <- table(NFL_Home$make_bet, prediction$.pred_class)
table_mat
```

```{r}
accuracy_Test <- sum(diag(table_mat)) / sum(table_mat)
sens_Test <- table_mat[2,2] / (table_mat[1,2] + table_mat[2,2])

accuracy_Test
sens_Test
```

# Away Analysis

## Logistic Regression 

### Final Cleaning

```{r}
NFL_Sample <- NFL_lines_split %>%
  select(-Index, -Final_line, -inserted, -public_ml, -public_spread, -public_team_under, -public_team_over) %>%
  na.omit() %>%
  group_by(id, book_id, Location) %>%
  sample_n(1)

NFL_Away <- NFL_Sample %>%
  filter(Location == 'away')

NFL_Away = subset(NFL_Away, select = -c(id,book_id, league, Location) )

```

### Model Setting

```{r}
# Make sure you set reference level (the outcome you are NOT interested in)
NFL_Away <- NFL_Away %>%
  mutate(outcome = relevel(factor(make_bet), ref='No')) #set reference level

NFL_cv10 <- vfold_cv(NFL_Away, v = 10)

# Logistic Regression Model Spec
logistic_spec <- logistic_reg() %>%
    set_engine('glm') %>%
    set_mode('classification')

# Recipe
logistic_rec <- recipe(make_bet ~ money_line + spread + total + team_total, data = NFL_Away, family = binomial('logit'), maxit = 100) %>%
    step_normalize(all_numeric_predictors()) %>% 
    step_dummy(all_nominal_predictors())

# Workflow (Recipe + Model)
log_wf <- workflow() %>% 
    add_recipe(logistic_rec) %>%
    add_model(logistic_spec) 

# Fit Model to Training Data
log_fit <- fit(log_wf, data = NFL_Away)
```

### Checking Model

```{r}
# Print out Coefficients
log_fit %>% tidy()

# Get Exponentiated coefficients + CI
log_fit %>% tidy() %>%
  mutate(OR.conf.low = exp(estimate - 1.96*std.error), OR.conf.high = exp(estimate + 1.96*std.error)) %>% # do this first
  mutate(OR = exp(estimate))
```

### Predicting on all Data
```{r}
# Make soft (probability) predictions
predict(log_fit, new_data = NFL_Away, type = "prob")

# Make hard (class) predictions (using a default 0.5 probability threshold)
predict(log_fit, new_data = NFL_Away, type = "class")
```

### Predicting on Training Data

```{r}
# Soft predictions
logistic_output <-  NFL_Away %>%
  bind_cols(predict(log_fit, new_data = NFL_Away, type = 'prob')) 

# Hard predictions (you pick threshold)
logistic_output <- logistic_output %>%
  mutate(.pred_class = make_two_class_pred(.pred_No, levels(outcome), threshold = .60)) #Try changing threshold (.5, 0, 1, .2, .8)

# Visualize Soft Predictions
logistic_output %>%
  ggplot(aes(x = outcome, y = .pred_Yes)) +
  geom_boxplot() + 
  geom_hline(yintercept = 0.60, color='red') +  # try changing threshold
  labs(y = 'Predicted Probability of Outcome', x = 'Observed Outcome') +
  theme_classic()
```

### Evaluating Model

```{r}
# Confusion Matrix
logistic_output %>%
  conf_mat(truth = outcome, estimate = .pred_class)

log_metrics <- metric_set(sens, yardstick::spec, accuracy) # these metrics are based on hard predictions

#sens: sensitivity = chance of correctly predicting second level, given second level (Yes)
#spec: specificity = chance of correctly predicting first level, given first level (No)
#accuracy: accuracy = chance of correctly predicting outcome

logistic_output %>% 
  log_metrics(estimate = .pred_class, truth = outcome, event_level = "second") # set second level of outcome as "success"
```

### ROC Curve

```{r}
logistic_roc <- logistic_output %>% 
    roc_curve(outcome, .pred_Yes, event_level = "second") # set second level of outcome as "success"

autoplot(logistic_roc) + theme_classic()
```

### CV Evaluation of Model

```{r}
# CV Fit Model
log_cv_fit <- fit_resamples(
    log_wf, 
    resamples = NFL_cv10,
    metrics = metric_set(sens, yardstick::spec, accuracy, roc_auc),
    control = control_resamples(save_pred = TRUE, event_level = 'second'))  # you need predictions for ROC calculations

collect_metrics(log_cv_fit) #default threshold is 0.5
```

## Random Forest Analysis

### Building Forest

```{r}
# Model Specification
rf_spec <- rand_forest() %>%
  set_engine(engine = 'ranger') %>% 
  set_args(mtry = NULL, # size of random subset of variables; default is floor(sqrt(number of total predictors))
           trees = 1000, # Number of trees
           min_n = 2,
           probability = FALSE, # FALSE: get hard predictions (not needed for regression)
           importance = 'impurity') %>% # we'll come back to this at the end
  set_mode('classification') # change this for regression

# Recipe
data_rec <- recipe(make_bet ~ spread + total + team_total + money_line, data = NFL_Away)

# Workflows
data_wf_mtry2 <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 2)) %>%
  add_recipe(data_rec)

## Create workflows for mtry = 12, 74, and 147

data_wf_mtry4 <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 4)) %>%
  add_recipe(data_rec)

"data_wf_mtry6 <- workflow() %>%
  add_model(rf_spec %>% set_args(mtry = 6)) %>%
  add_recipe(data_rec)"

```

### Fitting Models

```{r}
NFL_Away <- NFL_Away %>%
  na.omit()

set.seed(121) # make sure to run this before each fit so that you have the same 1000 trees
data_fit_mtry2 <- fit(data_wf_mtry2, data = NFL_Away)

```

```{r}

# Fit models for 12, 74, 147
set.seed(121) 
data_fit_mtry4 <- fit(data_wf_mtry4, data = NFL_Away)

"set.seed(121)
data_fit_mtry6 <- fit(data_wf_mtry6, data = NFL_Home)"
```

```{r}
# Custom Function to get OOB predictions, true observed outcomes and add a user-provided model label
rf_OOB_output <- function(fit_model, model_label, truth){
    tibble(
          .pred_class = fit_model %>% extract_fit_engine() %>% pluck('predictions'), #OOB predictions
          make_bet = truth,
          label = model_label
      )
}

#check out the function output
#rf_OOB_output(data_fit_mtry6,6, NFL_Home %>% pull(make_bet))
rf_OOB_output(data_fit_mtry4,4, NFL_Away %>% pull(make_bet))
rf_OOB_output(data_fit_mtry2,2, NFL_Away %>% pull(make_bet))
```

### Evaluating Models

```{r}
# Evaluate OOB Metrics

data_rf_OOB_output <- bind_rows(
    #rf_OOB_output(data_fit_mtry6,6, NFL_Away %>% pull(make_bet)),
    rf_OOB_output(data_fit_mtry4,4, NFL_Away %>% pull(make_bet)),
    rf_OOB_output(data_fit_mtry2,2, NFL_Away %>% pull(make_bet))
  
)


data_rf_OOB_output %>% 
    group_by(label) %>%
    accuracy(truth = factor(make_bet), estimate = .pred_class)
```

```{r}
data_rf_OOB_output %>%
  group_by(label) %>%
  accuracy(truth = factor(make_bet), estimate = .pred_class) %>%
  ggplot(aes(x = label, y = .estimate)) +
  geom_point() +
  geom_path()+
  scale_x_continuous(breaks = c(2, 4, 6))
```

### Evaluating Best Forest

```{r}
data_fit_mtry4
```
```{r, warning=FALSE}
mtry2Conf <- rf_OOB_output(data_fit_mtry2,2, NFL_Away %>% pull(make_bet)) %>%
    conf_mat(truth = make_bet, estimate= .pred_class)

mtry4Conf <- rf_OOB_output(data_fit_mtry4,4, NFL_Away %>% pull(make_bet)) %>%
    conf_mat(truth = make_bet, estimate= .pred_class)

mtry2Conf <- as.data.frame.matrix(mtry2Conf$table)
mtry4Conf <- as.data.frame.matrix(mtry4Conf$table)

mtry2Conf
mtry4Conf
```

### Variable Importance

```{r}
model_output <-data_fit_mtry4 %>% 
    extract_fit_engine() 

model_output %>% 
    vip(num_features = 30) + theme_classic() #based on impurity

model_output %>% vip::vi() %>% head()
```

```{r}
model_output2 <- data_wf_mtry4 %>% 
  update_model(rf_spec %>% set_args(importance = "permutation")) %>% #based on permutation
  fit(data = NFL_Away) %>% 
    extract_fit_engine() 

model_output2 %>% 
    vip(num_features = 30) + theme_classic()


model_output2 %>% vip::vi() %>% head()

```

## Decision Tree

### Fitting Tree

```{r}
ct_spec <- decision_tree() %>%
  set_engine(engine = 'rpart') %>%
  set_args(cost_complexity = 0.0022,  #default is 0.01 (used for pruning a tree)
           min_n = NULL, #min number of observations to try split: default is 20 [I think the documentation has a typo and says 2]  (used to stop early)
           tree_depth = NULL) %>% #max depth, number of branches/splits to get to any final group: default is 30 (used to stop early)
  set_mode('classification') # change this for regression tree


data_rec <- recipe(make_bet ~ money_line + spread + total + team_total , data = NFL_Away)

data_wf <- workflow() %>%
  add_model(ct_spec) %>%
  add_recipe(data_rec)

fit_mod <- data_wf %>% # or use tune_grid() to tune any of the parameters above
  fit(data = NFL_Away)
```

### Visualizing Tree

```{r}
# Plot the tree (make sure to load the rpart.plot package first)
fit_mod %>%
  extract_fit_engine() %>%
  rpart.plot()

# Get variable importance metrics 
# Sum of the goodness of split measures (impurity reduction) for each split for which it was the primary variable.
fit_mod %>%
  extract_fit_engine() %>%
  pluck('variable.importance')
```

```{r}
# Hard (class) prediction
prediction <- predict(fit_mod, new_data = NFL_Away, type = "class")
```

```{r}
table_mat <- table(NFL_Away$make_bet, prediction$.pred_class)
table_mat
```

```{r}
accuracy_Test <- sum(diag(table_mat)) / sum(table_mat)
sens_Test <- table_mat[2,2] / (table_mat[1,2] + table_mat[2,2])

accuracy_Test
sens_Test
```